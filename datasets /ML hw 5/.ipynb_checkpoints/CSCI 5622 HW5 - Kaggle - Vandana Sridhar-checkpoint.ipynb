{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#      CSCI 5622 Assignment 5, Kaggle Contest - Code and Description\n",
    "\n",
    "**Name: Vandana Sridhar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Load the test and train datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']\n",
    "traindata = pd.read_csv(\"train.data\",names=names)\n",
    "testdata = pd.read_csv(\"test.data\",names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Preprocessing using Label Binarizer, separating string data from numerical data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = traindata.drop(\"income\", axis=1)\n",
    "y = traindata[\"income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest=testdata.drop(\"income\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binarizer = LabelBinarizer()\n",
    "y = y_binarizer.fit_transform(y).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberfeatures = []\n",
    "stringfeatures = []\n",
    "\n",
    "for f in range(x.values.shape[1]):\n",
    "    if type(traindata.values[0, f]) is str:\n",
    "        stringfeatures.append(traindata.columns.values[f])\n",
    "    else:\n",
    "        numberfeatures.append(traindata.columns.values[f])\n",
    "\n",
    "class feature_elimination(): #Setup for the pipeline.Transformations on a column of data.\n",
    "    \n",
    "    f_names = list()\n",
    "    \n",
    "    def __init__(self, f_names):\n",
    "        self.f_names = f_names\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x[self.f_names].values\n",
    "\n",
    "    \n",
    "class feature_binarization(): #binarization for each feature\n",
    "    b = []\n",
    "    \n",
    "    def  __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, x, y=None): \n",
    "        no_of_features = x.shape[1]\n",
    "        for i in range(no_of_features):\n",
    "            a = LabelBinarizer()\n",
    "            a.fit(x[:, i])\n",
    "            self.b.append(a)\n",
    "        \n",
    "        return self\n",
    "            \n",
    "        \n",
    "    def transform(self, x): #does binarization, returns a matrix of binary features\n",
    "        mat = np.empty((0, 0))\n",
    "        \n",
    "        for f in range(len(self.b)):\n",
    "            if (f == 0):\n",
    "                mat = self.b[f].transform(x[:, f])\n",
    "            else:\n",
    "                mat = np.concatenate((mat, self.b[f].transform(x[:,f])),axis=1)\n",
    "\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Built a Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vandanasridhar/Documents/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/vandanasridhar/Documents/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/vandanasridhar/Documents/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "numberPip = Pipeline([('selector', feature_elimination(numberfeatures)),('std_scaler', StandardScaler())])\n",
    "stringPip = Pipeline([ ('selector', feature_elimination(stringfeatures)),('preprocessing_step', feature_binarization())])\n",
    "Pip_create = FeatureUnion(transformer_list=[('numberpipeline', numberPip),('stringpipeline', stringPip)])\n",
    "\n",
    "xtrain_new = Pip_create.fit_transform(x)\n",
    "xtest_new = Pip_create.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"new dataset \\n \", xtrain_new[0, :])\n",
    "#print(\"new testset \\n\", xtest_new[0, :])\n",
    "#print(Xtrain_new.shape)\n",
    "#print(Xtest_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Running a classifier and building the model, obtained predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of estimators = 150, 200, 300, 500 ..... \n",
    "#classifier1 = AdaBoostClassifier( n_estimators= 150, learning_rate = 1)\n",
    "#classifier1 = AdaBoostClassifier( n_estimators= 200, learning_rate = 1)\n",
    "#svc=SVC(probability=True, kernel='rbf')\n",
    "#classifier1 = AdaBoostClassifier( n_estimators= 200, base_estimator=svc, learning_rate = 1)\n",
    "#classifier1 = AdaBoostClassifier( n_estimators= 250, learning_rate = 1)\n",
    "#classifier2 = GradientBoostingClassifier( n_estimators= 160, learning_rate = 0.1)\n",
    "#classifier2 = GradientBoostingClassifier( n_estimators= 190, learning_rate = 0.1)\n",
    "classifier2 = GradientBoostingClassifier( n_estimators= 150, learning_rate = 0.1)\n",
    "model = classifier2.fit(xtrain_new, y)\n",
    "ytest_predict = model.predict(xtest_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Document predictions to file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1 = pd.DataFrame(ytest_predict,columns=['category']).to_csv('prediction7_gbc150.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Description Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> My best score for this assignment was 87.387% accuracy on Kaggle. <br/>\n",
    "\n",
    ">**APPROACH 1** :\n",
    "\n",
    "> The first approach consisted of me encoding categorical data. So I saw that columns such as workclass, education, marital status,occupation, relationship,race, sex and native country have string data so I mapped each individual string to a number. I handled question marks as well and I mapped them to -1. Thr first classifier that I ran was the Decision Tree classifier with the maximum leaf nodes parameter set to 145. My first accuracy was 83.783% which was a good start.<br/>\n",
    "\n",
    ">**APPROACH 2** :\n",
    "\n",
    "> I read quite a bit on preprocessing data and building pipelines. So my next step was to use the LabelBinarizer from the sklearn.preprocessing module. LabelBinarizer binarises labels in a one vs all manner. It is a type of one-hot encoding and I used it for y labels. Using the fit_transform(), binary targets are transformed to a column vector. The .ravel() function returns a one dimensional array.<br/>\n",
    ">Next I iterated through the list and separated the string features and the numerical features.<br/>\n",
    ">I had to devise a pipeline for this problem. Pipelines have fit & transform functions where the training data is fit to a pipeline and the test data is transformed using the pipeline. Pipelines streamline processes and makes it easy to model a problem. Sklearn's pipeline provides this functionality.<br/>\n",
    "\n",
    ">The feature_elimination class has the declaration for the pipeline ie, the fit() and transform() functions. The feature_binarization class has the definitions for fit() and transform(). The fit function performs the binarization for each feature in a one vs all manner. The transform function concatenates and returns the final matrix with all binary features.<br/>\n",
    ">Two pipelines are built - one for number data, and one for string data. The parameters passed for the pipelines are the action - \"the select function\" and the object - number features/string features. For the numerical features, the StandardScaler() is used which is just the standardization of features. This is done by removing the mean and scaling it to unit variance. For string features, feature binarization is used.<br/>\n",
    ">Now to make a pipeline from both pipelines, FeatureUnion is used to join them together and the new pipeline has a list of  feature transformers mentioned above.<br/>\n",
    ">I used the final pipeline to fit the training data and then transform the test data. The new preprocessed data is under the names xtrain_new and xtest_new respectively.<br/>\n",
    "\n",
    ">**CLASSIFIERS**\n",
    "\n",
    ">I used the descision tree classifier at first and then I moved on to Adaboost and GradientBoostingClassifier from the sklearn ensemble module.<br/>\n",
    "> I experimented mostly on the number of estimators for both algorithms. I used the default learning rate for both.<br/>\n",
    "> For Adaboost, I kept increasing the number of estimators starting from 150 to 250. My accuracy shot from 84.520% to 86.752%. <br/>\n",
    ">Another method I tried was by using the entire training set for one attempt and I split the data into train and test for the other attempt. I saw no improvement on the accuracy so I decided to continue using the entire dataset for the model.<br/>\n",
    ">I finally used the GradientBoostingAlgorithm with 150 estimators which finally got my accuracy to 87.387% on Kaggle.<br/>\n",
    ">I did try varying the estimators on Gradient boosting a couple times but accuracies either remained stagnant or dropped to a lower value. <br/>\n",
    "\n",
    ">**CONCLUSION**\n",
    "\n",
    ">Hence, by utilizing preprocessing steps, building a pipeline, using the GradientBoosting Algorithm with number of estimators = 150, I got my best score of 87.387%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
